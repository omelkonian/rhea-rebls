\documentclass[sigplan,review,anonymous]{acmart}
\settopmatter{printfolios=true,printccs=false,printacmref=false} %% max submission space

%% Conference information
\acmConference[PL'18]{ACM SIGPLAN Conference on Programming Languages}{January 01--03, 2018}{New York, NY, USA}
\acmYear{2018}
\acmISBN{}
\acmDOI{}
\startPage{1}
%% Copyright
\setcopyright{none}
%% Bibliography style
\bibliographystyle{ACM-Reference-Format}
%% Citation style
\citestyle{acmnumeric}  % acmauthoryear
%% Recommended packages
\usepackage{booktabs}   % formal tables
\usepackage{subcaption} % complex figures with subfigures/subcaptions
% Math
\usepackage{amsfonts}
% Tikz
\usepackage{tikz}
\usepackage{smartdiagram}
\usetikzlibrary{arrows,mindmap,trees,fit,backgrounds,decorations.pathreplacing,calc,positioning}
\usepackage{environ}
\makeatletter
\newsavebox{\measure@tikzpicture}
\NewEnviron{scaletikzpicturetowidth}[1]{%
  \def\tikz@width{#1}%
  \def\tikzscale{1}\begin{lrbox}{\measure@tikzpicture}%
  \BODY
  \end{lrbox}%
  \pgfmathparse{#1/\wd\measure@tikzpicture}%
  \edef\tikzscale{\pgfmathresult}%
  \BODY
}
\makeatother
% Colors
\usepackage{xcolor}
\colorlet{myrd}{ACMRed}
\colorlet{mygr}{ACMGreen}
\colorlet{mybl}{ACMLightBlue}
\colorlet{mybr}{ACMOrange}
\colorlet{myye}{ACMYellow}
% Images
\graphicspath{ {images/} }
% Code
\usepackage{minted}
% Extra
\usepackage{enumitem}
\usepackage{tabularx}
\usepackage{adjustbox}
\usepackage{url}

\begin{document}\sloppy
\input{MACROS.tex}

\title[RHEA]{RHEA: A Reactive, Heterogeneous, Extensible and Abstract Framework for Dataflow Programming}
\subtitle{}

%% Orestis
\author{Orestis Melkonian}
\orcid{0000-0003-2182-2698}
\affiliation{
  \department{Information and Computing Sciences}
  \institution{Utrecht University}
  \city{Utrecht}
  \country{The Netherlands}
}
\email{o.melkonian@uu.nl}

%% Angelos
\author{Angelos Charalambidis}
\orcid{0000-0001-7437-410X}
\affiliation{
  \department{Institute of Informatics and Telecommunications}
  \institution{NCSR ``Demokritos''}
  \city{Athens}
  \country{Greece}
}
\email{acharal@iit.demokritos.gr}

\begin{abstract}
The dataflow computational model enables writing highly
parallel programs, to be deployed on a heterogeneous network, in a concise and
readable way. The main advantage is the fact that the system can be conceptually
separated into several independent components that can be run in parallel and
deployed on different machines. Therefore, concurrency and distribution is
implicit and little or no responsibility is given to the programmer. The
framework proposed in this thesis constitutes the underlying system that make
this style of programming possible in JVM-based languages (e.g. Java, Scala,
Closure), while at the same time making it easy to integrate other technologies
that rely on the PubSub model, in order to move away from imperative languages
and enter a higher level of abstraction. Particular emphasis was put on three
domains, namely \textit{Big Data}, \textit{Robotics} and \textit{IoT}.
\end{abstract}

%% 2012 ACM Computing Classification System (CSS) concepts
%% Generate at 'http://dl.acm.org/ccs/ccs.cfm'.
\begin{CCSXML} <ccs2012> <concept>
<concept_id>10011007.10010940.10010971.10010972.10010545</concept_id>
<concept_desc>Software and its engineering~Data flow
architectures</concept_desc> <concept_significance>500</concept_significance>
</concept> <concept>
<concept_id>10011007.10011006.10011008.10011009.10011016</concept_id>
<concept_desc>Software and its engineering~Data flow languages</concept_desc>
<concept_significance>500</concept_significance> </concept> </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Data flow architectures}
\ccsdesc[500]{Software and its engineering~Data flow languages}
%% End of generated code


%% Keywords
\keywords{dataflow programming,
          stream processing,
          functional reactive programming (FRP),
          distributed systems,
          declarative languages,
          implicit concurrency,
          node placement}

\maketitle

\section{Introduction} \label{sec:introduction}

A typical application in Robotics or Internet of Things (IoT) needs to timely and
continuously respond to time-varying external sensory data. This fact makes the
reactivity of these applications imperative. Typically, a programmer of a robotic
application has to deal with the asynchronous callbacks in conventional imperative
programming languages in oder to implement tedious and often error-prone behaviours
that should comply with the reactive requirements.

%\paragraph{Why FRP is popular and many systems embrace it}
A promising and relatively recent proposal for simplyfing the implementation of
reactive applications is the \emph{functional reactive programming} (FRP) \cite{fran}.
FRP was originally proposed as a framework for developing graphical user interfaces
but the key high-level abstractions can be also applied in multiple domains that
require reactive applications. Essentially, FRP make heavy use of higher-order
functional operators to define a dataflow network of processing nodes. The merit


For example, several proposals exist for robotic applications~\cite{arrows_robots}.

The high-level abstraction of FRP have been proven useful in various applications
such as the development of graphical user interface~\cite{fran} and various
implementations of FRP emerge.

and several other proposals exist for Robotics applications~~\cite{arrows_robots}
%\paragraph{There are many domains that need reactive apps but have their own
% established architectures and protocols that makes it hard to employ a brand new FRP system}
% such domains are robotics and internet of things.

\paragraph{Dataflows in Robotics}

In control theory, which is the main background theory used in robotics, most
architectures and/or algorithms are represented as dataflow diagrams for the
sake of clarity and intuition. Translating these diagrams into common
"imperative" software is not an easy task and is usually the source of bugs.
Thus, having a dataflow execution model will nullify the need for such a
translation.

Specifically, most robotic applications follow the \textit{Robot Perception
Architecture (RPA)}, where inputs to system are the robot's sensors, which are
then processed by a dataflow graph, whose output is given as commands to the
robot actuators.

Moreover, robotics typically involve several different robotic systems, whose
combination is even more challenging. If each individual system is represented
as a dataflow graph, composing them together is as trivial as connecting inputs
with outputs, which is not the case in a traditional architecture, which is not
component-based.

% \paragraph{To make things worse: integration of systems and heterogeneity}

\paragraph{Data versus Computation}

A common problem in heterogeneous systems is that different representations of
the same entities/data-types coexist in the same software and, as a consequence,
pure computational tasks are intermingled with data-converting tasks. This makes
the code less readable and harder to maintain and understand. In the dataflow
execution model, where the program is modelled as directed graph of data flowing
between operations, there is a clear separation of these two aspects as data
(edges) are completely decoupled from computation (nodes). This motivation is
strengthened even more, when cross-machine communication is included, and apart
from converting data from one representation to another, serialization(i.e.
conversion to bytes) is also mandatory.

\paragraph{Contributions}
Our main contribution is the design and implementation of a framework for
dataflow programming to be deployed anywhere, ranging from low-performance
robots and sensors to clusters of computer and even the Cloud.

The main idea is to provide the programmer with a different execution model, the
dataflow model, which allows for a more abstract way of thinking and has the
advantage of exposing opportunities for parallelism (amongst CPU cores) and
distribution (amongst computational machines), which can then be automatically
realised by the "intelligent" underlying system.

Therefore, the programmer will be able to utilize available computational
resources without any effort, while at the same time reducing development
time/cost and maintaining a much cleaner and easier-to-refactor software system.
Resource utilization may appear in the form of faster execution (i.e. by
concurrently doing computations on multiple machines) or more robust
error-handling (i.e. by using backup machines to rerun nodes that were hosted
on a faulty machine).

\paragraph{Structure of the paper}
The rest of the paper is structured as follows. Section~\ref{sec:approach}.
Section~\ref{sec:related} discusses the related work. Finally it concludes
with future directions.

\section{Background} \label{sec:background}

\subsection{The dataflow computational model} The increased interest in
parallelism during the 70's gave rise to the dataflow execution model, which is
an alternative to the classical "von-Neumann" model. In the dataflow model,
everything is represented in a dataflow graph, where nodes are independent
computational units  and edges are communication channels between these units. A
node/unit is fired immediately when its required inputs are available and
therefore no explicit control commands are needed for execution. Figure
\ref{fig:nat} shows a dataflow graph enumerating the set $\mathbb{N}$ of natural
numbers.

\mydiag{nat}{Natural numbers}

In the dataflow graph above, we can discern three types of nodes: sources, which
do not have any incoming edge and act as value generators to initiate
computation, sinks, which do not have any outgoing edges and inner nodes, which
transform one or more incoming streams and redirect their output to other nodes.
The \textit{zero} node just produces a stream with a single value 0 and then
terminates. \textit{Concat} produces a single stream by concatenating the stream
produced by \textit{zero} and \textit{increment}, while \textit{increment}
transforms its input stream by adding one to its values. Finally, the sink node
displays the result, which is the stream of natural numbers.

Streams can be infinite, such as the stream produced by \textit{concat} because
it is the concatenation of a single-value stream and an infinite one. Moreover,
the graph is cyclic as \textit{concat} feeds input to \textit{increment} and
vice versa. The most interesting fact is that there nodes are independent and
therefore can run in parallel. For instance, while \textit{increment} is
processing value 5 (i.e. to produce value 6), the previous result (i.e. value 5)
passes through \textit{concat} to reach the sink node, which can concurrently
process it to display it.

The main advantage of the dataflow model is its implicit parallelism, deriving
from the fact that the computational units are totally independent and therefore
can be executed in parallel. A possible single-machine implementation could
represent edges as in-memory data storage, whereas a multi-machine one could
represent them as channels between TCP sockets, allowing communication across
the network. Its great flexibility and composability makes it a good candidate
for the underlying architecture of a framework with a high level of abstraction.

\subsection{Functional reactive programming}

A relatively recent programming paradigm is \textit{Functional Reactive
Programming (FRP)}, which provides a conceptual framework for implementing
reactive (i.e. time-varying and responding to external stimuli) behaviour in
\textit{hybrid systems} (i.e. containing both continuous and discrete
components), such as robots, in functional programming languages.

To implement such systems in conventional imperative languages, one must use
asynchronous \textit{callbacks} (i.e. each change is handled by a registered
\textit{callback} function). Although this solution is satisfactory for simple
schemes, more complex scenarios eventually lead to highly incoherent code
structure, often called \textit{spaghetti code}, in the sense that control
rapidly moves between disconnected parts of the system, similar to the notorious
\textit{GOTO} command. This phenomenon stems from the unary nature of
\textit{callback} functions, which requires some kind of "internal plumbing" in
order to achieve   mechanisms for handling combination of changes (e.g. when
multiple changes occur simultaneously). \textit{FRP} provides a solution to this
shortcoming of \textit{callback} functions, because changes are represented as
variables (\textit{signals}), which can be passed as parameters to arbitrary
functions, called \textit{signal functions}.

\textit{FRP} first appeared as a composable library for graphic animations
~\cite{fran}, but quickly evolved into a generic paradigm
~\cite{survey_frp,real_frp,pushpull_frp}. Moreover, extensive research has
investigated \textit{FRP} as a framework for robotics
~\cite{arrows_robots,lambda_in_motion}.

Although appealing at first, \textit{FRP} was not appropriate for systems  with
real-time constraints, due to uncontrollable time- and space- leaks
~\cite{event_frp}. The solution was a generalization of monads called
\textit{arrows}~\cite{arrows}, which provided the necessary guarantees that the
aforementioned common errors do not occur. For instance, to calculate
a robot's x-coordinate with the formula $x = 1/2 \int (vr + vl) \cos\theta$
one can write the following \textit{FRP} code that uses the \textit{arrow notation}~\cite{arrows_notation}:
\hssm{code/frp2.hs}

The main advantages of \textit{FRP} are its close correspondence to
mathematics~\cite{survey_frp}, which make it an ideal framework for modelling
real-time systems, and its concise representation of time-varying values via
\textit{signals}.

\paragraph{The Reactive Streams Standard}
The \textit{Reactive Streams Standard (RSS)} is an initiative
to provide a standard for asynchronous stream processing with non-blocking back
pressure. This encompasses efforts aimed at runtime environments (JVM and
JavaScript) as well as network protocols\cite{rss}.

\textit{RSS} defines two minimal interfaces for the roles of \textit{Subscriber}
and \textit{Publisher}\site{http://www.reactive-streams.org/reactive-streams-1.0.0-javadoc/}.
A \textit{Subscriber} implementation should define
reactions to observed values, including normal and erroneous termination,
whereas a \textit{Publisher} implementation should accept requests from
\textit{Subscribers} and start emitting values to them.

\subsection{Robotics and IoT Middlewares}

\paragraph{Publish-Subscribe model}

\textit{Publish/Subscribe (PubSub)} is a messaging pattern that became popular
due to the loose coupling of its components, suited for the most recent
large-scale distributed applications.

There is no point-to-point communication and no synchronization.
\textit{Publishers} advertise messages of a given type to a specific message
class or \textit{topic} that is identified by a \textit{keyword}, whereas
\textit{subscribers} listen on a specific \textit{topic} without any knowledge
of who the publishers are. The component responsible for relaying the messages
between machines and/or processes and finding the cheaper dissemination method
is called the \textit{message broker}.

\paragraph{ROS}
\textit{ROS} is an open-source middleware for robot software, which emphasizes
large-scale integrative robotics research cite{ROS}. It provides a \textit{thin}
communication layer between heterogeneous computers, from robots to mainframes
and it has been widely adopted by the research community around the world, due
to its flexibility and maximal support of reusability through packaging and
composability. It provides a compact solution to the development complexity
introduced by complex robot applications that consist of several modules and
require different device drivers for each individual robot.

It follows a peer-to-peer network topology, implemented using a topic-based
\textit{PubSub} messaging protocol and its architecture reflects many sound
design principles. Another great property of \textit{ROS} is that it is
language-agnostic, meaning that only a minimal specification language for
message transaction has been defined, so contributors are free to implement
small-size clients in different programming languages, with \textit{roscpp} and
\textit{rospy} being the most widely used ones.

A typical development scenario is to write several \textit{nodes}, that
subscribe to some topics and, after doing some computation, publish their
results on other topics. The main architectural issue here is that subscribing
is realized through asynchronous callback functions, so complicated schemes
easily lead to unstructured code, which obviously lead to unreadable and
hard-to-maintain code. Our approach gives a solution to the aforementioned
problem.

\paragraph{Internet of things - MQTT}

The birth of the Internet gave rise to a concept called \textit{Internet of
Things (IoT)}, which is essentially the ability of many heterogeneous devices,
ranging from low-cost sensors to vehicles with embedded electronics, to collect
data and exchange it amongst themselves using the Internet. This gave rise to
smart grids, smart homes and eventually smart cities.

The development of such systems though, due to their heterogeneity, is rather
complex and costly. Typical software architectures were not meant to be used in
such environments and therefore new tools and concepts needed to be invented.
Recent development of a variety of middleware frameworks, showed that a standard
protocol of communication is imperative along with supporting
tools~\cite{iot_middleware}. The most widely spread protocol is \textit{MQTT},
which follows the \textit{PubSub} messaging pattern and provides a very minimal
communication layer in order not to put a strain on the resource-bounded
system~\cite{mqtt}.

For instance, an \textit{IoT} application could connect to some sensors by
subscribing to their corresponding topics, taking decisions that would result in
some commands to some actuators, by publishing to their corresponding topics.

Fortunately, the dataflow model seems to be rather fitting for these
scenarios~\cite{iot_dataflow}, as every node in the graph is completely
independent, and consequently can be any "\textit{thing}". This useful property
of the model makes it a good architectural choice for such applications. The
only thing to consider is how these things will communicate in a standard way,
so as to be able to add new types of \textit{things} and integrate it in an
effortless way to an existing dataflow network.

\section{Requirements} \label{sec:requirements}

The design was heavily influenced by principles set out by the FRP and dataflow
models.

\paragraph{Reactive}
The system should be \textit{reactive}, as close as possible to the definition
of the Reactive Manifesto~\cite{manifesto}.

The system should be \textit{responsive}, meaning it should be able to handle
time-sensitive scenarios if at all possible. This is the cornerstone of
usability and utility, but more than that, it enables quick error-detection and
error-handling.

The system should be \textit{resilient}, meaning it is able to recover robustly
and gracefully after a failure, due to the fact that nodes in the dataflow graph
are completely independent and recovery of each one can be done in isolation.
Another thing to note here is that special error messages are built-in and make
it very easy to propagate errors between \textit{components}, in case the error-
handling part of a component is decoupled from the computational logic. This
leads to much more robust architectures for large-scale systems, where fault-
tolerance is mission-critical.

The system should be \textit{elastic}, meaning it will adjust itself depending
on the available resources and demanded workload. For instance, the granularity
of the graph (i.e. number of nodes) is adjusted so as to match a heuristic-based
value (e.g. total number of threads).

The system should be \textit{message-driven}, meaning it relies solely on
asynchronous message-passing for inter-component communication leading to loose
coupling, isolation, location transparency and the error propagation mentioned
above. Location transparency is critical to preserve the semantics whether on a
single host or a machine cluster.

\paragraph{Heterogeneous}
One of the major concerns while designing the framework was the ability to
deploy it anywhere, from low-cost robots to mainframes. Obviously, such
attribute would require a very flexible runtime environment. To satisfy this
requirement, the strategy design pattern was used for evaluation, meaning that
the core system only builds the internal representation of the dataflow graph
and partitions it across the available computational resources. From there
onwards, each partial graph can be evaluated by a different
\textit{EvaluationStrategy} (see Section \ref{sec:implementation}), which could
interpret it using a specific streams library or even compile into CUDA code for
execution on a GPU.

Figure \ref{fig:heterogen} illustrates a simple example of a robot application
pipeline, where input to the dataflow graph is what the robot's camera senses
and, after some image processing and some computation-heavy decision making, a
command to an actuator of the robot is executed. Orange nodes are deployed on
the robot's on-board computer, the green node is deployed on an off-board GPU
and the red node is deployed on the main server.

\mydiag{heterogen}{Heterogeneity pipeline}

\paragraph{Extensible}
As the work described in this thesis is quite fundamental and ambitious, it
seemed highly unlikely that it would reach closure. Therefore, careful
consideration was taken to compose the system of different independent modules,
which could easily be extended/modified, allowing many future contributions.

With that concept in mind, generality and abstraction were heavily emphasized
during both the design and the implementation process. We can say now we are
satisfied with the level of abstraction the core system has reached and hope the
stressful refactoring that the framework went through will blossom in the form
of future contributions.

\paragraph{Abstract}
The framework is \textit{abstract} in terms of implementation details, as it is
completely agnostic of any machine-specific requirements. It is designed as a
unifying conceptual base for further extensions and careful consideration was
taken not to restrict in any aspect, architectural or not. This was achieved by
making many parts of the core system pluggable, allowing for easy refactoring on
most of its internal functionality. Moreover, the internal graph representation
does not include information on how a node is executed, but only on its
semantics.

\section{Approach} \label{sec:approach}

This section presents the framework's main characteristics and capabilities.

\paragraph{System architecture}
The \textsc{RHEA} ecosystem consists of several
clearly separated modules, whose interconnection is illustrated in figure
\ref{fig:architecture}.

\myimage{architecture}{0.3}{System architecture}

The user writes a program in the provided stream language, which constructs a
dataflow graph internally. Afterwards, using information about the available
resources in the network, the constructed graph is optimized (i.e. in terms of
performance, communication cost and node placement). The optimized graph is then
distributed across the available machines for evaluation, maybe using a
different technique each time.

\paragraph{Supported dataflow graphs}
The kind of dataflow graph that can be expressed using the framework's stream
language are directed cyclic graphs with possibly many inputs and outputs.

\paragraph{The Stream data type}
The data channels (i.e. edges of the dataflow graph) are represented using the
\textit{Stream} data type, which is parametric, meaning that it can emit values
of any data type, whether built-in or user-defined. The stream produced may
terminate, successfully or erroneously, or even be infinite.

\paragraph{Graph construction syntax}
The construction of the internal dataflow graph is always implicit, through a
rich set of operators on the Stream data type. Each \textit{Stream} object
contains internally a dataflow graph of type \textit{FlowGraph}, which is only
to be accessed and manipulated by the internal module, evaluation strategies and
optimizers. Therefore, an application developer only needs to work with the
\textit{Stream} type.

Source nodes are constructed using built-in functions of type \textit{Stream}.
For instance, \textit{Stream.just(1, 2, 3)} produces the stream that emits just
the values 1, 2 and 3. The return variable of these creation function is an
object of type \textit{Stream}.

Processing nodes can be divided into two classes: \textit{single input} ones and
\textit{multiple input} ones.

\textit{Single input} nodes are inserted into an existing Stream object, by
calling an operator on that object. Figure \ref{fig:singleinput} shows an
example of a single input node, namely that of \textit{map}, which transforms
the input stream (i.e. just the values 1, 2 and 3) by applying a user-defined
function to every emitted value (i.e. $f(x)=x+1$).

\codefig{singleinput}{\jvs{code/singleinput.java}}{Single input processing node}

\textit{Multiple input} nodes are constructed by built-in function that take as
argument already existing Stream objects. Figure \ref{fig:multipleinput} shows
an example of a multiple input node, namely that of \textit{zip}, which
transforms the input streams (i.e. two stream that emit the values 1..10) by
applying a user-defined function to each emitted pairs of values (i.e.
$f(x,y)=x+y$). Here we also see another stream creation function, namely
\textit{Stream.range}.

\codefig{multipleinput}{\jvs{code/multipleinput.java}}{Multiple input processing node}

The variables returns by all processing nodes are \textit{Stream} objects. These
objects can be reused in different parts of the graph to enable splitting a
node's output to different processing nodes or outputs. Figure \ref{fig:split}
shows such an example, where the \textit{filter} operator only emits values for
which the given function returns true.

\codefig{split}{\jvs{code/split.java}}{Split example}

Cycle construction is a bit trickier, as no direct manipulation of the internal
graph is permitted. Cycles are constructed using the \textit{loop} operator,
which is a single input processing node. It requires a function that, given an
input stream, constructs a subgraph that redirects its output to that input,
therefore creating a feedback loop. Figure \ref{fig:natt} shows an example of
the \textit{loop} operator to represent the natural numbers, just as the graph
shown earlier in figure \ref{fig:nat}. The \textit{concat} operator is a
multiple input node that concatenates its input streams.

\codefig{natt}{\jvs{code/natt.java}}{Cyclic example}

\paragraph{Evaluation syntax and semantics}
To evaluate a given dataflow graph and do something with its output values, we
need to call the \textit{subscribe} method of the \textit{Stream} object. The
argument passed to \textit{subscribe} is either a user-defined action (i.e.
function with side-effects) or an object implementing the \textit{Subscriber}
interface.

The execution is completely asynchronous, hence the order of stream declaration
and evaluation does not matter at all.

\section{Implementation} \label{sec:implementation}

Since extensibility is a major design priority, most individual critical
components are defined using the \textit{Strategy design pattern}, isolating the desired
functionality in a separate \textit{interface} and allowing the system to select the
appropriate instantiating classes at runtime. Figure \ref{fig:core} illustrates all the
pluggable components of the framework around the core, which are normally deployed in separate libraries
and for which default implementations are already provided by the current implementation.
\mydiag{core}{The \textsc{RHEA} ecosystem}

\paragraph{Internal representation}
For representing the internal structure of the dataflow graph, the
\textit{JGrapht} open-source Java library was used, which provides many graph
data structures and common graph-theoretic algorithms~\cite{jgrapht}.

\paragraph{Notifications}
Every value passed through the framework's
\textit{streams} is wrapped inside a \textit{Notification} object, which
discriminates stream values into three categories: \textit{onNext} (when the
stream provides a regular value), \textit{onError} (when an error occurs) and
\textit{onComplete} (when the stream completes its output).

\paragraph{External Input-Output}
In order to make the framework easy to integrate with other stream and/or
dataflow technologies, every input/output node should implement the
interfaces that \textit{RSS} defines. This also enables users to define new
types of sources or sinks, in order to integrate the framework with other
general technologies (e.g. system events, HTTP requests, PubSub implementations,
etc).

A sink node (output) should implement the Subscriber interface, which
essentially defines three methods corresponding to reactions to a
\textit{Notification}, one for each of the categories mentioned above.

A source node (input) should implement the Publisher interface, which defines a
single method \textit{subscribe(Subscriber)}, where a Subscriber requests the
Publisher to start emitting values.

Many existing technologies provide these interfaces, or at least adapters from
their internal representations, and therefore they are very easy to be
integrated to the framework.

\subsection{Evaluation}

Every primitive operator corresponds to an expression implementing the
\textit{Transformer} interface and a complete dataflow is defined by a
\textit{Stream} variable and an object implementing the \textit{Output} interface,
which can be either an \textit{Action}, a \textit{Sink} or a list of these.
An \textit{EvaluationStrategy} just takes the \textit{Stream} variable and its
corresponding \textit{Output} and executes it, however desired.
The strategies we have implemented so far follow:
\begin{description}[style=nextline]
\item[RxJavaEvaluationStrategy] Uses
rxjava~\cite{rxjava}, which is a famous and well-
maintained library for asynchronous programming using the \textit{Observable}
type, which is very close, semantically, to our \textit{Stream} type.

\item[RosEvaluationStrategy] Integrates the \textit{ROS} middleware into the
framework. This strategy's job is to set up a \textit{ROS} client and configure every
\textit{RosTopic} used within the dataflow that needs to be evaluated to use
this client. After that, evaluation is propagated to a generic strategy (e.g.
rxjava).

\item[MqttEvaluationStrategy] Integrates the MQTT middleware into the framework,
in the same way \textit{ROS} is intergrated. \end{description}

\subsection{Distribution}

An evaluation strategy executes the requested dataflow graph in a single
machine, without concern about distribution and resource utilization.

For distribution and cluster management, one needs to implement
the \textit{DistributionStrategy} interface by adjusting the granularity (i.e. size) of
the graph to evaluate to fit the available resources (see \textit{Optimization} section)
and partition it across all computational resources, maybe using different evaluation strategies.

\paragraph{Hazelcast}
The default \textit{DistributionStrategy} shipped with the framework uses
the \textit{Hazelcast}~\cite{hazelcast} library to discover and
manage multiple machines and used its internal decentralized \textit{PubSub} model to communicate
intermediate results across the network. Figure \ref{fig:partition1} illustrates
the partitioning of a dataflow graph over several machines, where each machin
- except the last one - outputs its result to a \textit{Hazelcast} topic, from which
another machine gets its input.

\twofig{partition1}{partition2}{Partitioning}

\paragraph{Machine configuration}
According to the distribution strategy being used, the available machines will
require a certain initial configuration. For the \textit{Hazelcast} case, a
little piece of setup code needs to be executed on every member of the cluster,
which is together with the main \textit{Strategy} class. Moreover, helpful
information can also be added at this step, such as number of CPU cores. It is
the distribution strategy's responsibility to ensure that this information is
properly distributed and handled.

Apart from this initial configuration, the distribution strategy needs to enable
members to declare certain skills that they possess, which are required by
specialized nodes. For instance, a source node emitting values from a
\textit{ROS} topic must be executed on a machine having \textit{ROS} installed,
in order to set up a \textit{ROS} client. In the \textit{Hazelcast} case, these
skills are just \textit{strings} and are declared in the initialization code of
each machine separately.

\paragraph{Serialization}
As communication between machines across a network is mandatory, data types
emitted through the streams must be serialized on departure and de-serialized on
arrival at each machine. For this reason, each \textit{DistributionStrategy}
must be configured with a class implementing the \textit{Serializer} interface,
but we also provide a default one that covers most datatypes. Figure \ref{fig:serialization}
depicts the serialization process in more detail.

\mydiag{serialization}{Serialization process}

\section{Optimizations} \label{sec:optimization}

This section describes three stages of optimization the dataflow graph goes
through before being evaluated, which are illustrated in Figure \ref{fig:optimization}.
The purpose of the optimization graph is to achieve better performance and
utilization of the available resources.

\mydiag{optimization}{Optimization stages}

\subsection{Proactive filtering}

The first optimization stage is a heuristic one, based on the fact that if a
filter operation can be moved earlier (i.e. closer to source nodes) while
preserving the original semantics, then there will be benefit concerning
computational cost and cross-machine communication overhead. The figures below
illustrate one representative example of each general class of
graph transformation.

\optimization{1}{maptake}{Take/skip/distinct before map}
\optimization{1}{mapfilter}{Filter before map}
\optimization{3}{concatdistinct}{Filter/distinct before concat/merge}

\subsection{Granularity adjustment}

Different nodes of the dataflow graph will be executed on a separate
thread/process. The fact that graphs can grow very big, for instance when
programming a swarm of robots, poses a problem when available
computational resources are limited. For this reason, the second optimization
stage tries to adjust the granularity of the dataflow graph to a desired value,
which is normally the number of available threads amongst all machines.

To reach the desired granularity, the optimizer applies some semantic-preserving
transformation, as shown in the figures below.

\optimization{1}{mergemap}{Merge maps}
\optimization{1}{combmapfilter}{Combine map with filter}
\optimization{1}{combfilterexists}{Combine filter with exists}
\optimization{1}{combmapexists}{Combine map with exists}
\optimization{2}{combmapzip}{Combine map with zip}
\optimization{1}{combzipmap}{Combine zip with map}

In figure \ref{fig:mergemap} we merge two \textit{map} operations into
one \textit{map} operation that uses the composition of the two initial
functions, while in figure \ref{fig:combmapfilter} a \textit{map} followed
by a \textit{filter} is substituted by a more complex equivalent operation,
namely \textit{filterMap}. In figures \ref{fig:combfilterexists} and
\ref{fig:combmapexists} we apply some simple properties of the boolean functions
involved to decrease the number of nodes. Lastly, in figures \ref{fig:combmapzip} and
\ref{fig:combzipmap} we utilize function composition to embed \textit{map}
operations into \textit{zip} operations.

\subsection{Node placement}

After the first two passes, we have an optimized dataflow graph with fine-tuned
granularity. At this stage, nodes are mapped to tasks and are deployed across
the available machines, keeping resource utilization in mind.

If the desired granularity has not been reached yet, the
\textit{DistributionStrategy} applies fusion to pairs of tasks until it reaches
it, as shown in figure \ref{fig:fusion}.

\optimization{1}{fusion}{Task fusion}

The final decision to be made is where each of these newly constructed tasks
will be executed, although some of them need to necessarily be placed on
specific machines with certain skills.

Apart from these hard constraints, we need to minimize communication overhead.
For this purpose, one must implement the \textit{NetworkProfileStrategy} by providing
a way to calculate network distance between available machines, which is then fed as input
to the \textit{NodePlacement} optimizer.

\section{Applications} \label{sec:applications}

This section demonstrates some use-cases of the framework.

\subsection{Hamming numbers}

Consider the problem of enumerating the
\textit{Hamming numbers}, which are generated by the mathematical formula
$\mathbb{H} = 2^i3^j5^k$, where $i,j,k \in \mathbb{N}$. There is an intuitive
dataflow solution to the above problem, taken from the book of \textit{Lucid},
which is the first functional dataflow language~\cite{lucid}. Figure
\ref{fig:hamming} shows the dataflow graph with its corresponding \textsc{RHEA}
code.

\codefig{hamming}{\scalaxs{code/hamming.scala}}{Hamming numbers}

The code is written in Scala to utilize the \textit{Pimp my library} design
pattern~\cite{pimp}, which is used to easily add new functions to already
existing libraries, using Scala's \textit{implicit conversions} (line 28). In
the example above, we define two new Stream operators, namely \textit{multiply}
(line 10), which just multiplies the stream with a constant, and
\textit{mergeSort} (line 13), which produces an ordered stream given two ordered
streams as input. We also see the power of the \textit{loop} operator (line 2),
which allows us to define cycles in an effortless manner.

\subsection{Robot control panel}

This application concerns real-time monitoring of a robot, that is publishing
its information and sensor-data to \textit{ROS} topics, through a
\textit{graphical user interface (GUI)}.

The \textit{/camera/rgb} topic provides the frames of the robot's camera as
coloured images, while the \textit{/camera/depth} provides frames that provide
depth information. The \textit{/tf} topic publishes parent-child relations of
the internal topics of the robot's configuration, and finally the
\textit{/scan/} topic provides information from the robot's laser that gives
horizontal depth information in polar coordinates.

The GUI displays the laser data embedded on the camera stream, while allowing
for real-time face detection. Additionally, it displays the depth frames and the
\textit{tf} relations as a tree. Finally, a mock-up battery bar is displayed to
show-case the framework's ability for simulation. Figure \ref{fig:control_panel}
illustrates the dataflow solution to the above problem and its corresponding
\textsc{RHEA} code.

\codefig{control_panel}{\jvxs{code/control_panel.java}}{Robot control panel}

The implementation details (i.e. the visualization class and methods
\textit{faceDetect}(line 7), \textit{embedLaser}(line 8) and
\textit{toGray}(line 26)) are not shown for brevity's sake. It is evident that
this model of programming encourages a clean separation of concerns between the
individual components, namely between the sensor data manipulation and the
actual visualization on the GUI.

\subsection{Robot hospital guide}

As a final example, we will examine a more IoT-based application. Consider a
robot that guides patients to different parts of a hospital, such as the gym or
cafeteria. Assuming map localization, path finding and obstacle avoidance are
already implemented, there still remains a problem with calibrating the robot's
speed according to the patient's status.

To keep tract of the patient's distance from the robot, each patient carries a
smart-phone that acts as a \textit{bluetooth low-energy (BLE) beacon}. The robot
uses its bluetooth receiver to publish the distance from the signal source to an
\textit{MQTT} topic, which is then transformed by our stream application to
velocity commands for the robot, in the form of slowing down or speeding up.

The first module constitutes the main program logic, where a declared dataflow
graph acts as a stream transformation from beacon information to velocity
commands to the robot. Figure \ref{fig:hospital} shows the dataflow graph with
its corresponding \textsc{RHEA} code.

\codefig{hospital}{\jvxs{code/hospital.java}}{Robot hospital guide}

The second module just uses the \textit{ReactiveBeacons}
library~\cite{beacons} to get a stream of
beacon data via \textit{rxjava}, and then publishes it to a \textit{MQTT} topic,
which is the input of the first module. The corresponding \textsc{RHEA} code
follows:
\jvxs{code/rxbeacon.java}

This example clearly show-cases the framework's ability to combine different
technologies and act as a high-level, declarative coordination language.

\section{Related Work} \label{sec:related}

This section discusses related work in the fields of \textit{Big Data},
\textit{Robotics} and \textit{IoT}.

\subsection{Dataflow systems}

The necessity for implicit parallelism and distribution of more and more
applications, dealing with huge and/or complex data, has brought increasingly
more attention to the dataflow programming model. The main reason many
frameworks have adopted it, is the high level of abstraction it provides with
its declarative approach, making it simple to structure and maintain a complex
system, while at the same time not losing its expressive power.

%\paragraph{Spark}
A very well-known and well-adapted framework for scalable large-data processing
is Apache's \textit{Spark}~\cite{spark}. Although not a
dataflow framework, it was developed to overcome the shortcomings of the
\textit{MapReduce}, similar to \textit{FlumeJava}, by providing a much more
efficient and flexible runtime, offering a rich set of data-parallel operators ($\simeq 80$)
that can be used interactively from Scala, Python, Java or R.

It follows the same general approach as \textsc{RHEA}, in the sense that it is
completely generic and encourages domain-specific libraries to be built upon it.
For instance, \textit{MLib} is a library for machine learning and
\textit{GraphX} is a library for iterative graph algorithms, both stacked upon
\textit{Spark}.

%\paragraph{Akka}
Definitely one the most mature frameworks for distribution targeting the JVM,
\textit{Akka}~\cite{akka} is a toolkit and runtime for highly
concurrent, distributed and resilient message-driven applications. It is also
one of the founders of the \textit{Reactive Streams}~\cite{rss} initiative.

Its approach follows the \textit{Actor} model~\cite{actor}, where one perceives
abstract computational agents, called actors, that are distributed in space and
communicate with point-to-point messages that are buffered in a queue. In
reaction to a message, an actor can create more actors, make local decisions,
send more messages and determine how to respond to the next message received.

Similar to the problem of \textit{ROS} that our framework solved, which is the
inappropriate nature of callbacks for complex scenarios, \textit{Akka}
developers also felt the necessity for a more flexible and composable
programming model, so they developed the \textit{AkkaStreams} library, which
provides a convenient API for stream processing and also dataflow graph
construction with an interesting DSL.

The main reason \textsc{RHEA} offers a more flexible solution to \textit{ROS}
shortcomings than \textit{Akka}, is that \textit{Akka} is a pretty heavyweight
library, and consequently may prove over-abundant for simple use-cases. On the
other hand, \textsc{RHEA} offers the ability to choose between several
\textit{EvaluationStrategies} to match your application's needs, therefore a
simple application would just use a lightweight library like \textit{rxjava}.

\subsection{Unified dataflow languages}

%\paragraph{Cloud Dataflow}
Continuing the search for more expressive models, Google recently released the
\textit{Cloud Dataflow} framework~\cite{googledataflow},
which is an evolution of \textit{FlumeJava}~\cite{flumejava}, allowing cycles and
therefore incremental computation.

It is a completely domain-agnostic dataflow framework integrated with many other
closely-related technologies from Google, like Cloud Storage, Cloud PubSub,
Cloud Datastore, Cloud Bigtable and BigQuery.

It is open-source, offers fully automatic resource management that auto-scales
for optimal throughput and provides increased reliability and data consistency.
Moreover, it provides a unified programming model through its API, while
allowing data monitoring and demand-driven execution.

In contrast to \textsc{RHEA}, graphs constructed by \textit{Cloud Dataflow} are
designed to be deployed only on cloud infrastructures, and therefore no support
for complete heterogeneity is provided. In terms of network optimization, namely
node placement,  \textit{Cloud Dataflow} lets the cloud system targeted to make
all decisions, while \textsc{RHEA} profiles the network and decides
autonomously.

%\paragraph{dispel4py}
A less-known framework for Python is
\textit{dispel4py}~\cite{dispel4py}. It provides the ability
to describe abstract workflows for distributed data-intensive applications.

Similar to our \textit{EvaluationStrategy} concept, it allows different mappings
to enactment systems, such as MPI and Apache Storm.

Its main disadvantages are that it has only an API for Python and only allows
low-level specification of the graph's nodes, through the definition of
\textit{Processing Elements}. Therefore, it is inconvenient to compose larger
graphs from simpler ones and the source code becomes chaotic and difficult to
maintain.

\subsection{Robotics and IoT}

It is only natural that the dataflow model would make its way through the field
of robotics, as many behaviours in control theory are expressed as dataflow
diagrams.

%\paragraph{roshask}
\textit{Roshask}~\cite{roshask} is a binding from the Haskell programming language
to the basic \textit{ROS} interfaces. Like \textsc{RHEA}, the approach is to
overcome the shortcomings of \textit{ROS} callbacks by viewing topics as
streams. This allows for, and encourages, a higher level of abstraction in
robot programming, while making the fusing, transforming and filtering of
streams fully generic and compositional.

%\paragraph{Yampa}
\textsc{RHEA} and \textit{roshask} were heavily influenced by the work of Hudak's
group (Yale Haskell Group) on robot DSLs and FRP in general~\cite{fran,arrows_robots,lambda_in_motion}.
\textit{Yampa}~\cite{yampa} is a DSL embedded in Haskell
that realizes the FRP model, using arrows to minimize time-/space- leaks.

%\subsection{Internet of Things}

IoT applications often deal with much heterogeneity, due to the variety of
sources that different devices introduce. Therefore, a component-based approach
suits well to solve this problem and there are some dataflow frameworks that
follow that approach.

%\paragraph{Node-RED}
Another interesting \textit{IoT} tool for JavaScript following a dataflow
approach is \textit{Node-RED}~\cite{node-red}, which is a visual tool for wiring
together hardware devices, APIs and online services in new and interesting ways.

Applications called flows, are built immediately on a browser, and can be
deployed on the Cloud with just a single click. The main advantage of this tool
is that it encourages social development, due to the fact that flows are stored
in JSON format, which can be easily imported and exported for sharing with
others.

\subsection{Dataflow optimization}

%\paragraph{Stratosphere}
The frameworks discussed so far follow, more or less, an imperative approach,
which enables automatic distribution/concurrency by using immutable data
structures. \textit{Stratosphere}~\cite{stratosphere}, on the other hand, follows
a declarative programming approach similar to \textsc{RHEA}, which enables
writing highly parallel code directly from the language's semantics.

Apart from offering a language of a much higher abstraction level,
\textit{Stratosphere} has internalised several interesting and novel approaches
to optimization of dataflow graphs, especially concerning cyclic graphs (i.e.
incremental computation)~\cite{spinning}. These optimizations are generic, in the
sense that most frameworks can adopt them without much effort. Integrating these
optimization into \textsc{RHEA}, as future work, would certainly be of great
benefit to the performance of the system.

\subsection{Heterogeneous data processing}

%\paragraph{TensorFlow}

Another dataflow framework from Google is
\textit{TensorFlow}~\cite{tensorflow}, which is an open-source
polyglot library for machine learning and especially construction of neural
networks.

The interesting fact is that, although it started out as a rigid neural network
library, it quickly generalized to a dataflow construction library, much similar
to our own project, which started out as a robotics library.

Its main features are its portability to multiple computational architetures
(e.g. CPU, GPU, etc...) and multiple language APIs (e.g. C++, Python), although
its main advantage are its domain-specific operators for neural nets (i.e.
common subgraphs, auto-differentiation).

Through the edges/streams connecting the nodes, only a single but flexible data
type is allowed, namely the \textit{Tensor} type, which essentially is a multi-
dimensional array that usually represents features or weights. In contrast to
\textsc{RHEA}'s Streams, \textit{Tensors} cannot be infinite, mainly due to the
fact that their size is determined by the dimensionality of the problem being
solved, which is, in most cases, a fixed constant.

\section{Conclusions and Future Work} \label{sec:conclusions}

The framework described in this thesis was designed with extensibility in mind,
aiming to act as a fundamental basis, onto which various domain-specific
libraries or DSLs will rely in the future. To that end, a constant effort to
generalize and make components as abstract as possible was made.

The set of operators aided expressibility, making it possible to specify any
dataflow graph in a concise and readable manner. This disallowed optimizations
suitable for less expressive models (e.g. \textit{Map-Reduce}), but recent
research has shown that general dataflow topologies have optimization
opportunities that are yet to be found~\cite{blackbox}. A minimal optimization
stage has been implemented, which paves the path to more advanced optimization
techniques, such as those used in \textit{Naiad}~\cite{naiad} and
\textit{Stratosphere}~\cite{static_analysis}.

The extensible nature of \textsc{RHEA} allows for many meaningful extensions, such as
more evaluation/distribution strategies to support integration with other software ecosystems,
Moreover, the design of a block-based visual language interface would certainly make the
framework even more accessible to novice programmers and smooth the learning curve
associated with dataflow programming.

There are also significant shortcomings to the framework design and implementation that
could be addressed in future work.

For instance, \textit{dynamic reconfiguration} is needed to handle environments that are constantly changing.
A concrete contribution to the \textsc{RHEA} would be to integrate \textit{HotWave}~\cite{reconf_java},
which is an \textit{aspect-oriented programming (AOP)} framework that supports dynamic
(re)weaving of previously loaded classes. This would allow the user to specify the desired
adaptive behaviour for reconfiguring where nodes are executed, what operation they perform, and so forth.

Another major drawback is that network profiling - an integral part of node placement - is achieved via
calculation of \textit{round-trip time (RTT)}, which is naively expensive and may outweigh the benefits of
exploiting network proximity. A possible decentralized approach that we could employ would be the \textit{Vivaldi}
coordinate system~\cite{vivaldi}, which assigns synthetic coordinates to hosts
such that the distance between the coordinates of two hosts accurately predicts
the communication latency between them.

A last major drawback that ultimately needs to be addressed is fault-tolerance,
since there are no advanced methods for specifying behaviour for graceful
error-recovery. This is essential for large machine clusters, in which systems it is
certain that host failures and other faults will be a common occurrence. The
functional nature of the dataflow model enables fault-tolerance, in addition to
parallelism, due to the fact that a node can be moved to another machine for
execution, while preserving the original semantics.  This contribution path can draw
heavy influence from recent research on fault-tolerance for stream processing engines~\cite{borealis,wide_area},
which provide efficient models for availability and data recovery/consistency,
by using data replication and parallel recovery of lost state.

The applications demonstrated the framework's ability to provide a higher level
of abstraction, where the language only specifies how different components
coordinate, without knowledge of the implementation details. This is exactly
what \textit{Ziria} accomplishes in the domain of wireless systems
programming~\cite{ziria}. The driving force for both frameworks is that some
specific domains have fixated their methods on low-level programming, whereas
more satisfactory paradigms can solve many shortcomings.

This is a general notion in computer science, owning its existence to the fact
that the problems we are facing are getting increasingly more complex, while
resources meet certain realistic bounds, and therefore a higher abstraction
layer is mandatory for maintaining readability, efficiency and expressibility.


%% Bibliography
\bibliography{rhea}

\end{document}
